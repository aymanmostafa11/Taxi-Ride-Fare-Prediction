"""ML Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/aymanmostafa11/Taxi-Ride-Fare-Prediction/blob/main/ML%20Project.ipynb
"""

import gdown
gdown.download_folder('https://drive.google.com/drive/folders/1r9BARaPl-5odlOwCPE8LZJ1cFWRZGsYO?usp=sharing')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import linear_model
from sklearn import metrics
from helpers import Model , preProcessing

taxiRides = pd.read_csv('‪taxi‬‏/taxi-rides.csv')
weather = pd.read_csv('‪taxi‬‏/weather.csv')

print(f"Taxi Rides has {taxiRides.shape[0]} Rows and {taxiRides.shape[1]} Columns")
taxiRides.head()

print(f"Weather has {weather.shape[0]} Rows and {weather.shape[1]} Columns")
weather.head()

"""# Data Cleaning

## Taxi Rides
"""

taxiRides.info()

"""## Null"""

print("Null Values in columns")
taxiRides.isnull().sum()

"""## product_id and name"""

print(f"Value counts of 'product_id' feature\n")
taxiRides['product_id'].value_counts()

print(f"Value counts of 'name' feature\n")
taxiRides['name'].value_counts()



"""product_id and name represent the same feature so we can drop one of them

## Encoding Timestamps to date
"""

weatherDate = pd.to_datetime(weather['time_stamp'], unit='s').apply(lambda x: x.strftime(('%Y-%m-%d')))
taxiRidesDate = pd.to_datetime(taxiRides['time_stamp'], unit='ms').apply(lambda x: x.strftime(('%Y-%m-%d')))
weather['date'] = weatherDate
taxiRides['date'] = taxiRidesDate

taxiRides.head()

weather.head()

taxiRides.drop(['time_stamp'],axis = 1, inplace = True)
weather.drop(['time_stamp'],axis = 1, inplace = True)

"""## Joining Dataframes based on date"""

mergedData = pd.merge(taxiRides,weather.drop_duplicates(subset=['date', 'location']), how = 'left', left_on=['date', 'source'], right_on=['date', 'location'])

mergedData.head()

"""## Rain Feature"""

weather['rain'].isnull().sum() / weather['rain'].shape[0]

weather['rain'].hist()

"""Does null values of rain revolve around certain values?"""

print("Rows with null rain value statistics")
weather[weather['rain'].isnull()].describe()

print("Rows with non-null rain value statistics")
weather[weather['rain'].notna()].describe()

weather['rain'].value_counts()

print(f"Values of 0 in the rain feature {(weather['rain'] == 0).sum()}")

"""Rain feature nulls could indicate no rain"""

mergedData.shape

mergedData.isnull().sum()

mergedData.corr()

"""# Data Visualization"""

# to make the bars sorted
mergedData.sort_values('price', inplace=True)

plt.figure(figsize =(16,6))
sns.barplot(data=mergedData,x='name',y='price',hue='cab_type')
plt.show()

"""Apparently all *price* values of *Taxi* are missing, could all the missing values from *price* be from the *taxi* cab type? we need to verify this"""

taxiNullValues = mergedData[mergedData['price'].isnull()]['name'].value_counts()['Taxi']
totalNullValues = mergedData.isnull().sum()['price']
print(f"There are {taxiNullValues} price null values with Taxi as subtype from a total of {totalNullValues} \
: {taxiNullValues / totalNullValues * 100}%")

plt.figure(figsize =(16,6))
sns.lineplot(data=mergedData,x='distance',y='price')
plt.show()

"""Price as expected increases with distance, however there are some spikes that must have other factors contributing in"""

plt.figure(figsize = (13, 13))
plt.xticks(rotation = 60)
plt.yticks(rotation = 60)
plt.title('Correlation of data features')
sns.heatmap(mergedData.corr(), annot = True)
plt.show()

"""## Weather Graphs"""

plt.figure(figsize =(16,6))
sns.relplot(data=mergedData,x='temp',y='clouds')
plt.show()

plt.figure(figsize =(16,6))
sns.relplot(data=mergedData,x='temp',y='humidity')
plt.show()

plt.figure(figsize =(16,6))
sns.lineplot(data=mergedData,x='pressure',y='wind')
plt.show()

mergedData['cab_type'].hist()
plt.show()

mergedData['surge_multiplier'].hist()

"""# Preprocessing

## Encoding

###  <i>name</i> Feature
"""

preProcessing = preProcessing()
preProcessing.encode_name(mergedData['name'])

preProcessing.drop_adjust(mergedData)

"""### Other Features"""

nonIntegerColumns = [col for col in mergedData.columns if mergedData[col].dtypes == object]
print(f"Non Integer Columns : {nonIntegerColumns}")

preProcessing.encode(mergedData,nonIntegerColumns)

mergedData.dropna(axis=0, subset=['price'], inplace=True)

mergedData.isnull().sum()

"""## Rain feature

"""

mergedData['rain'].fillna(0,inplace=True)

mergedData['rain'].describe()

"""Referring to google:
<blockquote>Light rainfall is considered <b>less than 0.10 inches</b> of rain per hour. Moderate rainfall measures <b>0.10 to 0.30 inches</b> of rain per hour. Heavy rainfall is more than <b>0.30 inches</b>
 of rain per hour.</blockquote>
0 : no rain <br>
1 : light rain <br>
2 : mid rain <br>
3 : heavy rain (doesn't exist in the data)
"""

mergedData['rainType'] = 0

mergedData['rainType'][(mergedData['rain'] > 0) & (mergedData['rain'] < 0.1)] = 1

mergedData['rainType'][(mergedData['rain'] > 0.1) & (mergedData['rain'] < 0.3)] = 2

mergedData['rainType'].value_counts()

"""## Clouds engineering
making the assumption that clouds are on normalized [Okta Scale](https://polarpedia.eu/en/okta-scale/) that means values less than 0.1 are sunny days
"""

mergedData['sunnyDay'] = 0

mergedData['sunnyDay'][mergedData['clouds'] <= 0.1] = 1

mergedData['sunnyDay'].value_counts()

"""## Outliers"""

standardPrice = (mergedData['price'] - mergedData['price'].mean()) / mergedData['price'].std()

priceOutliers = mergedData[((standardPrice > 3.5) | (standardPrice < -3.5))]
print(len(priceOutliers))

print("Value counts of surge_multipliers for outlier prices\n")
priceOutliers['surge_multiplier'].value_counts()

print("Value counts of cab_class for outlier prices with normal surge_multipliers\n")
priceOutliers[priceOutliers['surge_multiplier'] == 1.0]['ride_class'].value_counts()

"""## Dimentionality Reduction"""

subsetOfData = mergedData[['temp','sunnyDay','rainType','wind','pressure','humidity']]
mergedData.drop(['temp','clouds','sunnyDay','rain','rainType','wind','pressure','humidity'],axis=1,inplace=True)

lowerDimensionWeatherData =preProcessing.reduceDimentionsOf(subsetOfData)
mergedData['weatherState'] = lowerDimensionWeatherData

plt.figure(figsize = (13, 13))
plt.xticks(rotation = 60)
plt.yticks(rotation = 60)
sns.heatmap(mergedData.corr(), annot = True)
plt.show()

"""# Model"""

dataFeatures = mergedData.drop(['price'],axis=1)
dataLabel = mergedData['price']

dataFeatures.head()

model = Model()

"""### First Model"""

splitData = model.splitData(dataFeatures,dataLabel)

model.fitLinearModel(splitData["trainFeatures"],
                     splitData["trainLabel"],
                     metrics.r2_score,
                     splitData["testFeatures"],
                     splitData["testLabel"])

"""### Second Model"""

polyDegree = 4
model.fitPolyModel(splitData["trainFeatures"],
                    splitData["trainLabel"],
                    polyDegree,
                    metrics.r2_score,
                    splitData["testFeatures"],
                    splitData["testLabel"])

model.crossValidateOn(linear_model.LinearRegression(),
                      dataFeatures,
                      dataLabel,
                      polyDegree,
                      metric = metrics.r2_score, k = 3)

